---
title: "CW"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(NbClust)
library(tictoc)
library(beepr)
library(flexclust)
library(corrplot)
library(dendextend)
library(ggplot2)
library(magrittr)

wine <- read.csv("https://github.com/jaewilson07/Hello-World/raw/master/Data%20Mining%20Coursework/Whitewine.csv")

```

## Explore and Format the imported Wine data
Although the wine dataframe imported properly, the Quality variable is not an ordinal factor as described
-- cast the Quality variable from INT to Factor

Plot counts of wine grouped by Quality to explore the distribution of quality levels.
-- only 7 out of 10 possible levels are (unevenly) represented in our dataset.
-- Assuming that subjective taste quality can be used to distinguish clusters of chemical attributes, there should be at most 7, more likely 5 clusters in our dataset.

-- To avoid leaking information about the outcme variable to our algorithm, we'll begin by first isolating the Quality variable from our dataset.
-- Because our chemical property variables are measured on different scales (eg. the range of Acidity is 3.8 : 14.2 while the range of Chlorides is only .009 : .346); we'll scale each variable by taking x - center (the variable mean) / standard deviation of the variable to prevent one variable from more strongly affecting the k-means distancing algorithm.



```{r formatWine}
#explore the wine dataframe
str(wine)

#Convert the Quality variable into an Ordinal Factor
qual_levels <- c(1:10)
wine$quality	<-factor(wine$quality, levels = qual_levels, ordered = TRUE)

#view the distribution of Quality in the existing dataset
ggplot(wine, aes(x = quality)) +
	geom_bar()

#isolate the quality columns
wine_quality <- wine$quality

#scale the data set
wine_train <- wine[,-12]
wine_train <- as.data.frame(scale(wine_train))

```

## Find a recommendation for cluster sizes
Use NbClust() function and it's 30 indices to find recommendations for number of clusters to use in the k-means algorithm.
-- We'll set our limits for cluster size recommendations at 2 and 7 clusters because given our earlier assumption, there shouldn't be more than 7 clusters.



```{r clusterWine}

#find recommendation for best number of clusters
tic()
numClust_wine <- NbClust(wine_train, method = "kmeans", min.nc =  2, max.nc = 7)
beep()
#store duration of NBClust
t_clust<- toc()
t_clust

#numClust_wine$Best.n[1,] stores a list of the number clusters recommended by each index
#find the top two (mode) wine cluster recommendations
topClust_Wine_Rec <- as.numeric(names(sort(-table(numClust_wine$Best.n[1,])))[1:2])
topClust_Wine_Rec

```

## runKmeans
Use kmeans() function to calculate 3 cluster permutations inl 7 (number of Quality levels represented in data) and the two most frequently occuring cluster recommendations.
```{r runKMeans}
paste(
	c("Run k-means using 7, the number of represented Quality groups.  Additionally run k-means with the following cluster sizes"
		, topClust_Wine_Rec[1]
		, topClust_Wine_Rec[2] ), collapse = ", ")

k_wine_10 <- kmeans(wine_train, 7)
k_wine_1 <- kmeans(wine_train, topClust_Wine_Rec[1])
k_wine_2 <- kmeans(wine_train, topClust_Wine_Rec[2])
```

## Analyze results

```{r analyze results}
tbwine <- table(wine[,12])
tb_ct_10 <- table(k_wine_10$cluster, wine[,12])
tb_ct_1 <- table(k_wine_1$cluster, wine[,12])
tb_ct_2 <- table(k_wine_2$cluster, wine[,12])

randIndex(tb_ct_10)
randIndex(tb_ct_1)
randIndex(tb_ct_2)
```


## perform a Hierarchical Cluster using 3 different distancing methods
We start our hierarchical clustering exercise by creating hclust models using single, complete and average distance methods.

As expected, the plot of the dendrograms are less informative because there are so many clusters as the hclust() algorithm combines the 4000+ observations.  

```{r hclust}
#hclust (agglomerative) single
hclust_sing <- hclust( dist(wine_train), method= "single")
dend_sing <- as.dendrogram(hclust_sing)
#hclust complete
hclust_compl <- hclust( dist(wine_train), method= "complete")
dend_compl <- as.dendrogram(hclust_compl)
#hclustaverage methods
hclust_avg <- hclust( dist(wine_train), method= "average")
dend_avg <- as.dendrogram(hclust_avg)

#Create dendogram visualization of all 3
layout(matrix(c(1:3),1,3))
plot(hclust_sing)
plot(hclust_compl)
plot(hclust_avg)
beep()

```


##Cut the dendogram tree
Although k-means and hierarchical clustering are clearly different approaches to unsupervised categorization of data, we'll recycle the previously recommended cluster size of from the nbclust(kmeans) algorithm to cut our dendogram trees.
```{r cutTree}
#cut the dendrograms and bind them to the wine_train dataset
cutclust_sing <- cutree(hclust_sing, k = topClust_Wine_Rec[2], order_clusters_as_data =  FALSE)
cutclust_compl <-  cutree(hclust_compl, k = topClust_Wine_Rec[2], order_clusters_as_data =  FALSE)
cutclust_avg <-  cutree(hclust_avg, k = topClust_Wine_Rec[2], order_clusters_as_data =  FALSE)

  
wine_train$hclust_sing <- as.factor(cutclust_sing)
wine_train$hclust_compl <- as.factor(cutclust_compl)
wine_train$hclust_avg <- as.factor(cutclust_avg)


plot(cutclust_sing)
plot(cutclust_compl)
plot(cutclust_avg)

#create a list of dendgrams
d <- dendlist(
		single = dend_sing,
		complete = dend_compl,
		average = dend_avg)
#cor.dendlist between each clustering result 
tic("dend")
cor.dendlist(d, "cophenetic")
beep()
tic_dend <-toc()
#Discuss the produced results after using the coorplot function.

h<- dendlist(hclust_avg)

plot(bestClust$Best)

str(bestClust)

```

