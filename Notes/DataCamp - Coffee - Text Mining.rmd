---
title: "Coffee - Text Analsyis"
author: "Jae Wilson"
date: "`r format(Sys.Date())`"
output: github_document
---

#Setup
```{r Setup , message = FALSE }
# install.packages("qdap", dependencies = TRUE)
# install.packages("tm", dependencies = TRUE)
# install.packages("wordcloud", dependencies = TRUE)
# install.packages("plotrix", dependencies = TRUE)
# install.packages("ggthemes", dependencies = TRUE)
# install.packages("RWeka" , dependencies = TRUE)

library(knitr)
library(magrittr)

library(qdap) #Quantitative Discourse Analysis Packag
library(tm) #text mining
library("wordcloud")
library("plotrix") #for pyramidcoud
library("dendextend")
library(ggplot2)
library(ggthemes)
library(RWeka)

library(dplyr)
library(tidytext)

```


```{r udfFunctions}

# create a custom reader for capturing metadata
custom_reader <- readTabular(mapping = list(content = "text", 
                                            id = "num", 
                                            author = "screenName", 
                                            date = "created"))

#create a function to apply transformations
#use content_transformer to inform tm_map that you'd like to 'transform content'
#NOTE: when using cleansing functions from tm library no content_transformer required()
clean_corpus_coffee <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwordLibraryCoffee)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  return(corpus)
}

clean_corpus_chardonnay <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwordLibraryChardonnay)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stemDocument, language = "english")
  return(corpus)
}

# Make tokenizer function 
tokenizer <- function(x) {
  NGramTokenizer(x, Weka_control(min=2, max=2))
}
```


```{r importData}
stopwordLibraryCoffee <- c(stopwords("en"), "coffee")
stopwordLibraryChardonnay <- c(stopwords("en"), "chardonnay")
#import and isolate tweets
coffee_tweets_df<- read.csv("https://raw.githubusercontent.com/jaewilson07/Hello-World/master/Datasets/DataCamp/CoffeeTweets.txt", stringsAsFactors = FALSE)
chardonnay_tweets_df <- read.csv("https://raw.githubusercontent.com/jaewilson07/Hello-World/master/Datasets/DataCamp/ChardonnayTweets.txt", stringsAsFactors = FALSE) %>%
  rename( num = X)

```

#Bag of Words

##Define Corpuses
```{r createVectorSourceCorpus}

coffee_tweets <- coffee_tweets_df$text
chardonnay_tweets <- chardonnay_tweets_df$text

# Make a vector source (list): coffee_source
coffee_vec_source <- VectorSource(coffee_tweets)
coffee_vec_corpus <- VCorpus(coffee_vec_source)


# Print out coffee_corpus
print(coffee_vec_corpus)

# Print data on the 15th tweet in coffee_corpus
print(coffee_vec_corpus[[15]])


# Print the content of the 15th tweet in coffee_corpus
print(coffee_vec_corpus[[15]][1])
#"@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! #Don'tDrinkNShoot"


#NOTE:  DataframeSource() will convert entire row into ONE DOCUMENT (retain independent tweets structure)
#NOTE : VectorSource(sample_df) will convert entire sample_df$data1 into one document (combine all variables)
#sample_df <- data.frame( data1 = c("Apples and bananas" , "I think i need to drink coffee"),
            # data2 = c( "Pears, plums and peaches", "This is how we suffer through bad classes, #Starbucks" )) %>%
#  VectorSource()
#print(sample_df)
```

```{r DataFrameCorpus}
# Make corpus which includes metadata by adding readerControl
coffee_df_corpus <- VCorpus( 
  DataframeSource(coffee_tweets_df),
  readerControl = list(reader = custom_reader)) %>%
  clean_corpus_coffee()

chardonnay_df_corpus <- VCorpus( 
  DataframeSource(chardonnay_tweets_df),
  readerControl = list(reader = custom_reader)) %>%
  clean_corpus_chardonnay()

# Print data
#str coffee_corpus
# NOTE: syntax for selecting one liste item
str(coffee_df_corpus[[1]])
print(coffee_df_corpus[[1]][1])
print(coffee_df_corpus[[1]][2])
```

##DTMs and Frequencies
```{r DTMsAndFrequencyPlots}

##DTM vs TDM simply transposes the length / width of the resulting matrix
(coffee_df_dtm <- DocumentTermMatrix(coffee_df_corpus))
coffee_m_dtm <- as.matrix(coffee_df_dtm)

(coffee_df_tdm <- TermDocumentMatrix(coffee_df_corpus))
coffee_m_tdm <- as.matrix(coffee_df_tdm)

chardonnay_df_tdm <-TermDocumentMatrix(chardonnay_df_corpus)
chardonnay_m_tdm <- as.matrix(chardonnay_df_tdm)

#there are 1000 documents and 3098 unique words in coffee_m
dim(coffee_m_dtm) 
dim(coffee_m_tdm) 


#plot simple bag of words (frequency of each word)
#using qdap library freq_terms()
#NOTE: inclusion of uninformative words
coffee_freq <- freq_terms(
  coffee_tweets, 
  top= 10)
plot(coffee_freq)

#NOTE: parameters to exclude Top200 stopwords as well words with fewer than 3 characters
coffee_freq <- freq_terms(
	coffee_tweets,
	top = 10,
	at.least = 3, #number of characters in vector
	stopwords = "Top200Words")
plot(coffee_freq)


#alternatively, use rowSums() on the coffee DTM matrix to generate a summary matrix
#NOTE: applcaition of sort() function on matrix
#NOTE: using stowords("en") library + Coffee
coffee_freq <- rowSums(coffee_m_tdm) %>%
	sort(decreasing = TRUE)
barplot(coffee_freq[1:10], col = "tan", las = 2)



wordcloud(coffee_df_corpus
          , max.words = 100
          , random.order = TRUE
          ,  colors = c("grey80", "darkgoldenrod1",  "tomato")
          )
```

##Stem Completion
Reduce the number of words by matching word variations to their stem/root word

```{r stemCompletion}
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly"

# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))
#stem document will consider each element of a vector as one word.
#use strsplit() to break a sentence into vectors of words

str(n_char_vec)
#chr [1:13] "In" "a" "complicated" "haste" "Tom" "rushed" "to" "fix" "a" ...

# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)
# [1] "In"      "a"       "complic" "hast"    "Tom"     "rush"    "to"     
# [8] "fix"     "a"       "new"     "complic" "too"     "complic"

# Print stem_doc
print(stem_doc)

comp_dict <- c("In", "a", "complicate", "haste", "Tom", "rush", "to", "fix", "a", "new", "too")

# Re-complete stemmed document: complete_doc
#the stemCompletion is a complete list of all the 'valid' words
complete_doc <-stemCompletion(stem_doc, comp_dict) 


# Print complete_doc
complete_doc


#UNFINISHED ATTEMPT TO CREATE A VALID STEMCOMPLETION DOCUMENT
#stem_coffee_corp <- tm_map(coffee_df_corpus, stemDocument, language = "en")
#stem_coffee_m_tdm <- stem_coffee_corp %>%
#  TermDocumentMatrix() %>%
#  as.matrix()

#all_coffee_words <- paste( rownames(coffee_m_tdm), collapse =  " " )
#all_coffee_stem <- paste( rownames(stem_coffee_m_tdm), collapse = " " )
# all_coffee_stem <- c(all_coffee_words, all_coffee_stem) %>%
#   VectorSource() %>%
#   VCorpus() %>%
#   TermDocumentMatrix()
# 
# all_coffee_stem <- as.matrix(all_coffee_stem)
# 
# head(all_coffee_stem, 100)
# 
# stemmed_coffee <- subset(all_coffee_stem , all_coffee_stem[,1] == 0 & all_coffee_stem[,2] >0)
# stemmed_coffee

```


##understanding color in visualization
```{r colorPalette}
# List the available colors
display.brewer.all()

# Create purple_orange
purple_orange <- brewer.pal(10, "PuOr")

# Drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]
```

##Visualizing Corpus

### Combine Corpus
Combine two corpus to see commonality
```{r combine Corpus}

#collapse into one big ass string
all_coffee_words <- paste( rownames(coffee_m_tdm), collapse =  " " )

#CODE VARIANT: using df, all_coffee_words <- paste(coffee_tweets, collapse = " ")

# Create all_chardonnay
all_chardonnay_words <- paste( rownames(chardonnay_m_tdm), collapse = " " )

# combine tweets into a 2 element vector
all_words <- c(all_coffee_words, all_chardonnay_words) %>%
  VectorSource() %>%
  VCorpus() %>%
  clean_corpus_chardonnay()

all_tdm <- TermDocumentMatrix(all_words) %>%
  `colnames<-` (c("coffee", "chardonnay"))

all_m_tdm <- as.matrix(all_tdm)

# Print a commonality cloud
commonality.cloud(all_m_tdm
  , colors = "steelblue1"
  , max.words = 100)


# print comparison cloud
comparison.cloud(all_m_tdm
  , colors= c("orange", "blue")
  , max.words = 50)

```

###Polarized Cloud
show top 25 words shared in each corpus
show the absolute difference between words represented in each corpus (top should have the least difference in word representation)
eg. Cup is common to both corpuses, but Chardonnay tweeters use cup MUCH MORE frequently than Coffee tweeters.

```{r PolarizedTagCloud}

# Create common_words as the subset of words that exist in both columns (freq count >1)
common_words_all <- subset(all_m_tdm, all_m_tdm[, 1] > 0 & all_m_tdm[, 2] > 0)

# Create difference as the sub
difference <- abs(common_words_all[, 1] - common_words_all[, 2])

# Combine common_words and difference
common_words_all <- cbind(common_words_all, difference)

rm(difference)

# Order the data frame from most differences to least
common_words_all <- common_words_all[order(common_words_all[, 3], decreasing = TRUE), ]

# Create top25_df
top25_df <- data.frame(x = common_words_all[1:25, 1], 
                       y =common_words_all[1:25, 2], 
                       labels = rownames(common_words_all[1:25, ]))

# Create the pyramid plot
pyramid.plot(top25_df$x, top25_df$y 
  ,labels = top25_df$labels
  , top.labels = c("Chardonnay", "Words", "Coffee")
  , gap = 1
  , space= .3
  , main = "Words in Common"
  , raxlab = NULL
  , laxlab = NULL
  , unit = NULL
  )
```

###Word Associations
```{r wordAssociation , message = FALSE}
# Word association
word_associate(coffee_tweets, match.string = c("barista"), 
               stopwords = c(Top200Words, "coffee", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "Barista Coffee Tweet Associations")
```


#additional skills for improving text mining

##understanding hclusts and dendrograms
```{r hclust}

rain <- data.frame( "city" = as.factor(c("Portland", "Cleveland", "Boston", "New Orleans"))
                    , "rainfall" = c( 39.1, 39.1, 43.8, 62.5))

str(rain)
# Create dist_rain
dist_rain <- dist(rain$rainfall)

# View the distance matrix
print(dist_rain)

# Create hc
hc <- hclust(dist_rain)

# Plot hc
plot(hc, labels = rain$city)
```

##Cleaning up dendrograms by reducing Sparsity
Limit the number of words in your TDM using removeSparseTerms() from tm. 

Why would you want to adjust the sparsity of the TDM/DTM?  TDMs and DTMs are sparse, meaning they contain mostly zeros. 
```{r}

```

* A good TDM has between 25 and 70 terms. 
* The lower the sparse value, the more terms are kept. The closer it is to 1, the fewer are kept. 

* This value is a percentage cutoff of zeros for each term in the TDM.

```{r sparsityInTM}
# Print the dimensions of tweets_tdm
dim(coffee_df_tdm)

# Create tdm1
coffee_sparse965 <- removeSparseTerms(coffee_df_tdm, sparse = .965) %>%
    as.matrix()

# Create tweets_tdm2
coffee_sparse975 <- removeSparseTerms(coffee_df_tdm, sparse= .975) %>%
  as.matrix()

# Create tweets_dist
coffee_sparse_dist975 <- dist(as.data.frame(coffee_sparse975))
coffee_sparse_dist965 <- dist(as.data.frame(coffee_sparse965))

# Create hc
hc975 <- hclust(coffee_sparse_dist975)
hc965 <- hclust(coffee_sparse_dist965)

plot(hc975)
plot(hc965)

# Create hcd
hc965_dend <- as.dendrogram(hc965)

# Print the labels in hcd
labels(hc965)

# Change the branch color to red for "marvin" and "gaye"
coffee_hcd<- branches_attr_by_labels(hc965_dend, c("starbuck", "portland") , col="red")

# Plot hcd with rectangles at k 5
plot(coffee_hcd)
rect.dendrogram(coffee_hcd, k=5, border ="grey50")
```


### using Associations to analyze data
Use findAssocs() function in the tm package. 

For any given word, findAssocs() calculates correlation with every other word in a TDM or DTM. 
* Scores range from 0 to 1. 
* Where 1 means that two words always appear together
* Minimum correlation values are often relatively low because of word diversity. (0.10 could demonstrate a strong pairwise term association)

```{r associations}

# Create associations
coffee_assos <- findAssocs(coffee_df_tdm, "venti", .2)
class(coffee_assos)
print(coffee_assos)
# View the venti associations

# Create associations_df
coffee_assos_df <- list_vect2df(coffee_assos)[, 2:3] %>%
  rename( correl = X3, word = X2)

# Plot the associations_df values
ggplot(coffee_assos_df, aes(x = correl, y = word)) + 
  geom_point(size = 3) + 
  theme_gdocs() +
  labs(title = "Word Associations with \'Venti\'", x = "Correlation", y ="Assoc. Word")
```

##Tokenizer

The default DTM is unigrams
* can use tokeinzer to create bi / tri grams (w two or more words per token).
* help extract useful phrases can lead to some additional insights
* improved predictive attributes for a machine learning algorithm.

Customized tokenizer() function can be passed into the TermDocumentMatrix or DocumentTermMatrix functions as an additional parameter
* Note: creates significantly larger DTMs

```{r tokenizer}

# Create bigram_dtm
coffee_df_dtm_bigrm <- DocumentTermMatrix(coffee_df_corpus, control = list(tokenize = tokenizer))

# Examine unigram_dtm
print(coffee_df_dtm)

# Examine bigram_dtm
print(coffee_df_dtm_bigrm)

# Create bigram_dtm_m
coffee_df_m_dtm_bigrm <- as.matrix(coffee_df_dtm_bigrm)

# Create freq
coffee_freq_bi <- colSums(coffee_df_m_dtm_bigrm)

# Create bi_words
coffee_bi_words <- names(coffee_freq_bi)
coffee_un_words <- names(coffee_freq)

# Examine part of bi_words
print(coffee_bi_words[2577:2587])

# Plot a wordcloud
par(mfrow=c(1,2))
wordcloud(coffee_un_words, coffee_freq, max.words =30)
wordcloud(coffee_bi_words, coffee_freq_bi, max.words= 30)
```

###Term Weights
Frequently occuring words (like coffee, or chardonnay) may skew or hide insights.
* change frequency weights to penalize words that occur too often
*TfIdf (term frequency inverse document frequency) is a common Term Weight modifier.  Where TF increases with frequency and is diminished by appearing in all documents

```{r TermWeights}

# Create tfidf_tdm
tfidf_tdm <- TermDocumentMatrix(coffee_df_corpus, control = list(weighting = weightTfIdf))

# Create tfidf_tdm_m 
tfidf_tdm_m <- as.matrix(tfidf_tdm)

# Examine part of tf_tdm_m
coffee_m_tdm[22, 5:60]

# Examine part of tfidf_tdm_m
tfidf_tdm_m[22, 5:60]
```

```{r}



# Choose the bing lexicon
get_sentiments("bing")

# Choose the nrc lexicon
get_sentiments("nrc") %>%
  count(sentiment) # Count words by sentiment
```



```{r SentimentLexicons}
# Choose the bing lexicon
get_sentiments("bing")

# Choose the nrc lexicon
get_sentiments("nrc") %>%
  count(sentiment) # Count words by sentiment
```

```{r}

```

